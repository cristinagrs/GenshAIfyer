{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import logging\n",
    "import itertools\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "import diffusers\n",
    "import transformers\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "from PIL.ImageOps import exif_transpose\n",
    "from torchvision import transforms\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import DistributedDataParallelKwargs, ProjectConfiguration, set_seed\n",
    "\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    FlowMatchEulerDiscreteScheduler,\n",
    "    SD3Transformer2DModel,\n",
    "    StableDiffusion3Pipeline,\n",
    ")\n",
    "from diffusers.training_utils import (\n",
    "    _set_state_dict_into_text_encoder,\n",
    "    cast_training_params,\n",
    "    compute_density_for_timestep_sampling,\n",
    "    compute_loss_weighting_for_sd3,\n",
    ")\n",
    "from peft import LoraConfig, set_peft_model_state_dict\n",
    "from peft.utils import get_peft_model_state_dict\n",
    "from diffusers.utils.torch_utils import is_compiled_module\n",
    "from diffusers.optimization import get_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "global_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts:  1102\n",
      "Number of characters:  56\n",
      "Number of imgs:  557\n",
      "Total dataset:  11140\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'genshin_dataset'\n",
    "train_df_path = os.path.join(dataset_path, 'dataset1_sd3_emb2.csv')\n",
    "train_df = pd.read_csv(train_df_path)\n",
    "prompts = train_df['description'].unique()\n",
    "print('Number of prompts: ', len(prompts))\n",
    "print('Number of characters: ', len(train_df['character'].unique()))\n",
    "print('Number of imgs: ', len(train_df['im_path'].unique()))\n",
    "print('Total dataset: ', len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenshinDataset(Dataset):\n",
    "    def __init__(self, data, args):\n",
    "        self.df = data\n",
    "        self.size = args.resolution\n",
    "        self.custom_instance_prompts = True # we use a customized dataset\n",
    "        \n",
    "        self.image_column = args.image_column\n",
    "        self.caption_column = args.caption_column\n",
    "        self.emb_column = args.embeddings\n",
    "        self.pooled_emb_column = args.pooled_embeddings\n",
    "\n",
    "        self.image_transforms = train_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),\n",
    "                transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "        self.num_instance_images = len(self.df)\n",
    "        self._length = self.num_instance_images\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        row = self.df.iloc[index]\n",
    "        class_image = Image.open(row[self.image_column])\n",
    "        class_image = exif_transpose(class_image)\n",
    "        if not class_image.mode == \"RGB\": class_image = class_image.convert(\"RGB\")\n",
    "\n",
    "        #instance_images, instance_prompt\n",
    "        example[\"instance_images\"] = self.image_transforms(class_image)\n",
    "        example[\"instance_prompt\"] = row[self.caption_column]\n",
    "\n",
    "        example[\"instance_emb\"] = torch.load(row[self.emb_column], weights_only=True)\n",
    "        example[\"instance_pooled_emb\"] = torch.load(row[self.pooled_emb_column], weights_only=True)\n",
    "        return example\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = [example[\"instance_images\"] for example in examples]\n",
    "    prompts = [example[\"instance_prompt\"] for example in examples]\n",
    "    embeddings = [example[\"instance_emb\"] for example in examples]\n",
    "    pooled_embeddings = [example[\"instance_pooled_emb\"] for example in examples]\n",
    "\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "    embeddings = torch.stack(embeddings)\n",
    "    embeddings = embeddings.to(memory_format=torch.contiguous_format).float()\n",
    "    pooled_embeddings = torch.stack(pooled_embeddings)\n",
    "    pooled_embeddings = pooled_embeddings.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "    batch = {\"pixel_values\": pixel_values, \"prompts\": prompts, \"embeddings\": embeddings, \"pooled_embeddings\": pooled_embeddings}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class TrainParams():\n",
    "    exp_name = 'sd3_exp4'\n",
    "\n",
    "    train_dataframe_path:str = train_df_path\n",
    "    train_dataframe = train_df #train data    \n",
    "    dataset_name:str = 'dataset1_genshin_dataset'\n",
    "    caption_column:str = 'description'\n",
    "    image_column:str = 'im_path'\n",
    "    embeddings:str = 'embeddings'\n",
    "    pooled_embeddings:str = 'pooled_embeddings'\n",
    "\n",
    "    output_dir:str = f\"models/{exp_name}\" #'models/train_dataset1_sd2'#'finetune_stable-diffusion-2-1'\n",
    "    pretrained_model_name_or_path:str = \"stabilityai/stable-diffusion-3-medium-diffusers\"\n",
    "    #\"stabilityai/stable-diffusion-2-1\"\n",
    "\n",
    "    cache_dir:str = os.path.join(output_dir, 'cache')\n",
    "    logging_dir:Path = Path(os.path.join(output_dir, 'logs'))\n",
    "    seed:int = 1991\n",
    "\n",
    "    ### specific new params..\n",
    "    precondition_outputs:int = 1 #Flag indicating if we are preconditioning the model outputs or not as done in EDM. This affects how model `target` is calculated.\n",
    "\n",
    "    weighting_scheme:str = \"none\" #choices=[\"sigma_sqrt\", \"logit_normal\", \"mode\", \"cosmap\", \"none\"] \n",
    "    #We default to the \"none\" weighting scheme for uniform sampling and uniform loss'\n",
    "    mode_scale:float = 1.29 #Scale of mode weighting scheme. Only effective when using the `'mode'` as the `weighting_scheme`\n",
    "    logit_mean:float = 0.0 #mean to use when using the `'logit_normal'` weighting scheme.\n",
    "    logit_std:float = 1.0 #std to use when using the `'logit_normal'` weighting scheme.\n",
    "\n",
    "\n",
    "    ## image params and transforms\n",
    "    resolution:int = 256\n",
    "    center_crop:bool = True #Whether to center crop the input images to the resolution. \n",
    "    #If not set, the images will be randomly cropped. \n",
    "    random_flip:bool = True\n",
    "    dataloader_num_workers:int=2\n",
    "\n",
    "    ## accelerator params\n",
    "    gradient_accumulation_steps:int = 1\n",
    "    mixed_precision:str = \"fp16\" #choices=[\"no\", \"fp16\", \"bf16\"]\n",
    "    report_to:str = \"tensorboard\"\n",
    "    gradient_checkpointing: bool = True\n",
    "\n",
    "    # model\n",
    "    revision:str=None\n",
    "    variant:str=\"fp16\"\n",
    "    #lora rank\n",
    "    rank:int = 4\n",
    "    lora_alpha:int = 16 #32    \n",
    "\n",
    "    #optimizer train params\n",
    "    learning_rate:float = 1e-4\n",
    "    text_encoder_lr: float = 5e-6\n",
    "    guidance_scale:float = 3.5 #the FLUX.1 dev variant is a guidance distilled model\n",
    "\n",
    "    scale_lr:bool = False\n",
    "    lr_scheduler:str = \"constant\" #Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\",\n",
    "                                #\"constant\", \"constant_with_warmup\"]'\n",
    "    lr_warmup_steps:int = 500\n",
    "    lr_num_cycles:int = 1 #Number of hard resets of the lr in cosine_with_restarts scheduler.\n",
    "    lr_power:float = 1.0 #Power factor of the polynomial scheduler\n",
    "\n",
    "    train_batch_size:int = 32 #16\n",
    "    sample_batch_size:int = 10\n",
    "    num_train_epochs:int = 30\n",
    "    max_train_steps:int = 5000\n",
    "\n",
    "    #### optimizer\n",
    "    optimizer:str = \"AdamW\" ##chose between adamw and prodigy\n",
    "    use_8bit_adam:bool = True   #Choose between 'epsilon' or 'v_prediction' or leave `None`. \n",
    "    #If left to `None` the default prediction type of the scheduler: `noise_scheduler.config.prediction_type` is chosen.\n",
    "    prediction_type:str = None \n",
    "    \n",
    "    ###\n",
    "    local_rank:int = 1\n",
    "\n",
    "    adam_beta1:float = 0.9\n",
    "    adam_beta2:float = 0.999\n",
    "    adam_weight_decay:float = 1e-2\n",
    "    adam_weight_decay_text_encoder:float = 1e-3\n",
    "    adam_epsilon:float = 1e-08\n",
    "\n",
    "    #coefficients for computing the Prodigy stepsize using running averages. If set to None, uses the value of square root of beta2. Ignored if optimizer is adamW\n",
    "    prodigy_beta3:float = None\n",
    "    prodigy_decouple:bool = True #Use AdamW style decoupled weight decay\n",
    "\n",
    "    #snr args\n",
    "    snr_gamma:float = None #SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0.\n",
    "    noise_offset:float = 0 #The scale of noise offset.\n",
    "\n",
    "    #other\n",
    "    validation_prompt:str=\"A girl with blue eyes, black hair and a bright smile. The girl wears a red dress and a ponytail with a flower decoration.\"\n",
    "    num_validation_images:int=4 #Number of images that should be generated during validation with `validation_prompt`.\n",
    "    validation_epochs:int = 1\n",
    "    max_train_samples:int=None\n",
    "\n",
    "    max_grad_norm:float = 1.0\n",
    "\n",
    "    #Choose between 'epsilon' or 'v_prediction' or leave `None`. \n",
    "    #If left to `None` the default prediction type of the scheduler: `noise_scheduler.config.prediction_type` is chosen.\n",
    "    prediction_type:str = None \n",
    "    \n",
    "    ###\n",
    "    local_rank:int = 1\n",
    "\n",
    "    ##save params\n",
    "    checkpointing_steps:int=200 # Save a checkpoint of the training state every X updates\n",
    "    checkpoints_total_limit:int = 10 # Max number of checkpoints to store.\n",
    "    \n",
    "    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    if env_local_rank != -1 and env_local_rank != self.local_rank:\n",
    "        self.local_rank = env_local_rank\n",
    "        \n",
    "args = TrainParams()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "with open(os.path.join(args.output_dir, 'args.json'), 'w') as file:\n",
    "    args_dict = vars(args)\n",
    "    del args_dict['logging_dir']\n",
    "    file.write(json.dumps(args_dict, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crisan/miniconda3/envs/dif_dev/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "08/20/2024 12:35:54 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=args.logging_dir)\n",
    "kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    mixed_precision=args.mixed_precision,\n",
    "    log_with=args.report_to,\n",
    "    project_config=accelerator_project_config,\n",
    "    kwargs_handlers=[kwargs],\n",
    ")\n",
    "\n",
    "# Disable AMP for MPS.\n",
    "if torch.backends.mps.is_available():\n",
    "    accelerator.native_amp = False\n",
    "\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state)#, main_process_only=False)\n",
    "if accelerator.is_local_main_process:\n",
    "    transformers.utils.logging.set_verbosity_warning()\n",
    "    diffusers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "    diffusers.utils.logging.set_verbosity_error()\n",
    "\n",
    "set_seed(args.seed)\n",
    "\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'max_shift', 'base_image_seq_len', 'max_image_seq_len', 'base_shift', 'use_dynamic_shifting'} was not found in config. Values will be initialized to default values.\n",
      "{'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n"
     ]
    }
   ],
   "source": [
    "# Load scheduler and models\n",
    "noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"scheduler\"\n",
    ")\n",
    "noise_scheduler_copy = copy.deepcopy(noise_scheduler)\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "transformer = SD3Transformer2DModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"transformer\")\n",
    "transformer.requires_grad_(False)\n",
    "vae.requires_grad_(False)\n",
    "\n",
    "weight_dtype = torch.float32\n",
    "if accelerator.mixed_precision == \"fp16\":\n",
    "    weight_dtype = torch.float16\n",
    "elif accelerator.mixed_precision == \"bf16\":\n",
    "    weight_dtype = torch.bfloat16\n",
    "\n",
    "vae.to(accelerator.device, dtype=torch.float32)\n",
    "transformer.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "if args.gradient_checkpointing: transformer.enable_gradient_checkpointing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add LoRA weights to attention layers\n",
    "transformer_lora_config = LoraConfig(\n",
    "    r=args.rank,\n",
    "    lora_alpha=args.lora_alpha,\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    ")\n",
    "transformer.add_adapter(transformer_lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap_model(model):\n",
    "    model = accelerator.unwrap_model(model)\n",
    "    model = model._orig_mod if is_compiled_module(model) else model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7a3480e42dd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create custom saving & loading hooks so that `accelerator.save_state(...)` serializes in a nice format\n",
    "def save_model_hook(models, weights, output_dir):\n",
    "    if accelerator.is_main_process:\n",
    "        transformer_lora_layers_to_save = None\n",
    "        text_encoder_one_lora_layers_to_save = None\n",
    "        text_encoder_two_lora_layers_to_save = None\n",
    "\n",
    "        for model in models:\n",
    "            if isinstance(model, type(unwrap_model(transformer))):\n",
    "                transformer_lora_layers_to_save = get_peft_model_state_dict(model)\n",
    "            else:\n",
    "                raise ValueError(f\"unexpected save model: {model.__class__}\")\n",
    "\n",
    "            # make sure to pop weight so that corresponding model is not saved again\n",
    "            weights.pop()\n",
    "\n",
    "        StableDiffusion3Pipeline.save_lora_weights(\n",
    "            output_dir,\n",
    "            transformer_lora_layers=transformer_lora_layers_to_save,\n",
    "            text_encoder_lora_layers=text_encoder_one_lora_layers_to_save,\n",
    "            text_encoder_2_lora_layers=text_encoder_two_lora_layers_to_save,\n",
    "        )\n",
    "\n",
    "accelerator.register_save_state_pre_hook(save_model_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.mixed_precision == \"fp16\":\n",
    "        models = [transformer]\n",
    "        # only upcast trainable parameters (LoRA) into fp32\n",
    "        cast_training_params(models, dtype=torch.float32)\n",
    "\n",
    "transformer_lora_parameters = list(filter(lambda p: p.requires_grad, transformer.parameters()))\n",
    "\n",
    "\n",
    "if args.scale_lr: args.learning_rate = (args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes)\n",
    "\n",
    "# Optimization parameters\n",
    "transformer_parameters_with_lr = {\"params\": transformer_lora_parameters, \"lr\": args.learning_rate}\n",
    "params_to_optimize = [transformer_parameters_with_lr]\n",
    "\n",
    "if args.use_8bit_adam:\n",
    "        try:\n",
    "                import bitsandbytes as bnb\n",
    "        except ImportError:\n",
    "                raise ImportError(\"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\")\n",
    "\n",
    "        optimizer_class = bnb.optim.AdamW8bit\n",
    "else:\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "\n",
    "optimizer = optimizer_class(params_to_optimize, betas=(args.adam_beta1, args.adam_beta2), weight_decay=args.adam_weight_decay, eps=args.adam_epsilon,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoaders creation:\n",
    "train_dataset = GenshinDataset(data=args.train_dataframe, args=args)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda examples: collate_fn(examples),\n",
    "    num_workers=args.dataloader_num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    args.lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,\n",
    "    num_training_steps=args.max_train_steps * accelerator.num_processes,\n",
    "    num_cycles=args.lr_num_cycles,\n",
    "    power=args.lr_power,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(transformer, optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if overrode_max_train_steps:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "# Afterwards we recalculate our number of training epochs\n",
    "args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if accelerator.is_main_process:\n",
    "    tracker_name = args.exp_name\n",
    "    accelerator.init_trackers(tracker_name)#, config=vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sigmas(timesteps, n_dim=4, dtype=torch.float32):\n",
    "    sigmas = noise_scheduler_copy.sigmas.to(device=accelerator.device, dtype=dtype)\n",
    "    schedule_timesteps = noise_scheduler_copy.timesteps.to(accelerator.device)\n",
    "    timesteps = timesteps.to(accelerator.device)\n",
    "    step_indices = [(schedule_timesteps == t).nonzero().item() for t in timesteps]\n",
    "\n",
    "    sigma = sigmas[step_indices].flatten()\n",
    "    while len(sigma.shape) < n_dim:\n",
    "        sigma = sigma.unsqueeze(-1)\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/20/2024 12:35:59 - INFO - __main__ - ***** Running training *****\n",
      "08/20/2024 12:35:59 - INFO - __main__ -   Num examples = 11140\n",
      "08/20/2024 12:35:59 - INFO - __main__ -   Num batches each epoch = 349\n",
      "08/20/2024 12:35:59 - INFO - __main__ -   Num Epochs = 15\n",
      "08/20/2024 12:35:59 - INFO - __main__ -   Instantaneous batch size per device = 32\n",
      "08/20/2024 12:35:59 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "08/20/2024 12:35:59 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "08/20/2024 12:35:59 - INFO - __main__ -   Total optimization steps = 5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c068c6cee93a4c7783ff88bf48514305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crisan/miniconda3/envs/dif_dev/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "08/20/2024 12:43:17 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-200\n",
      "Model weights saved in models/sd3_exp4/checkpoint-200/pytorch_lora_weights.safetensors\n",
      "08/20/2024 12:43:18 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-200/optimizer.bin\n",
      "08/20/2024 12:43:18 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-200/scheduler.bin\n",
      "08/20/2024 12:43:18 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-200/sampler.bin\n",
      "08/20/2024 12:43:18 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-200/scaler.pt\n",
      "08/20/2024 12:43:18 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-200/random_states_0.pkl\n",
      "08/20/2024 12:43:18 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-200\n",
      "08/20/2024 12:50:35 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-400\n",
      "Model weights saved in models/sd3_exp4/checkpoint-400/pytorch_lora_weights.safetensors\n",
      "08/20/2024 12:50:35 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-400/optimizer.bin\n",
      "08/20/2024 12:50:35 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-400/scheduler.bin\n",
      "08/20/2024 12:50:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-400/sampler.bin\n",
      "08/20/2024 12:50:35 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-400/scaler.pt\n",
      "08/20/2024 12:50:35 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-400/random_states_0.pkl\n",
      "08/20/2024 12:50:35 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-400\n",
      "08/20/2024 12:57:54 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-600\n",
      "Model weights saved in models/sd3_exp4/checkpoint-600/pytorch_lora_weights.safetensors\n",
      "08/20/2024 12:57:54 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-600/optimizer.bin\n",
      "08/20/2024 12:57:54 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-600/scheduler.bin\n",
      "08/20/2024 12:57:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-600/sampler.bin\n",
      "08/20/2024 12:57:54 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-600/scaler.pt\n",
      "08/20/2024 12:57:54 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-600/random_states_0.pkl\n",
      "08/20/2024 12:57:54 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-600\n",
      "08/20/2024 13:05:12 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-800\n",
      "Model weights saved in models/sd3_exp4/checkpoint-800/pytorch_lora_weights.safetensors\n",
      "08/20/2024 13:05:12 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-800/optimizer.bin\n",
      "08/20/2024 13:05:12 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-800/scheduler.bin\n",
      "08/20/2024 13:05:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-800/sampler.bin\n",
      "08/20/2024 13:05:12 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-800/scaler.pt\n",
      "08/20/2024 13:05:12 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-800/random_states_0.pkl\n",
      "08/20/2024 13:05:12 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-800\n",
      "08/20/2024 13:12:33 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-1000\n",
      "Model weights saved in models/sd3_exp4/checkpoint-1000/pytorch_lora_weights.safetensors\n",
      "08/20/2024 13:12:33 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-1000/optimizer.bin\n",
      "08/20/2024 13:12:33 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-1000/scheduler.bin\n",
      "08/20/2024 13:12:33 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-1000/sampler.bin\n",
      "08/20/2024 13:12:33 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-1000/scaler.pt\n",
      "08/20/2024 13:12:33 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-1000/random_states_0.pkl\n",
      "08/20/2024 13:12:33 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-1000\n",
      "08/20/2024 13:19:53 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-1200\n",
      "Model weights saved in models/sd3_exp4/checkpoint-1200/pytorch_lora_weights.safetensors\n",
      "08/20/2024 13:19:53 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-1200/optimizer.bin\n",
      "08/20/2024 13:19:53 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-1200/scheduler.bin\n",
      "08/20/2024 13:19:53 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-1200/sampler.bin\n",
      "08/20/2024 13:19:53 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-1200/scaler.pt\n",
      "08/20/2024 13:19:53 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-1200/random_states_0.pkl\n",
      "08/20/2024 13:19:53 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-1200\n",
      "08/20/2024 13:27:10 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-1400\n",
      "Model weights saved in models/sd3_exp4/checkpoint-1400/pytorch_lora_weights.safetensors\n",
      "08/20/2024 13:27:10 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-1400/optimizer.bin\n",
      "08/20/2024 13:27:10 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-1400/scheduler.bin\n",
      "08/20/2024 13:27:10 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-1400/sampler.bin\n",
      "08/20/2024 13:27:10 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-1400/scaler.pt\n",
      "08/20/2024 13:27:10 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-1400/random_states_0.pkl\n",
      "08/20/2024 13:27:10 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-1400\n",
      "08/20/2024 13:34:27 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-1600\n",
      "Model weights saved in models/sd3_exp4/checkpoint-1600/pytorch_lora_weights.safetensors\n",
      "08/20/2024 13:34:27 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-1600/optimizer.bin\n",
      "08/20/2024 13:34:27 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-1600/scheduler.bin\n",
      "08/20/2024 13:34:27 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-1600/sampler.bin\n",
      "08/20/2024 13:34:27 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-1600/scaler.pt\n",
      "08/20/2024 13:34:27 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-1600/random_states_0.pkl\n",
      "08/20/2024 13:34:27 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-1600\n",
      "08/20/2024 13:41:44 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-1800\n",
      "Model weights saved in models/sd3_exp4/checkpoint-1800/pytorch_lora_weights.safetensors\n",
      "08/20/2024 13:41:44 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-1800/optimizer.bin\n",
      "08/20/2024 13:41:44 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-1800/scheduler.bin\n",
      "08/20/2024 13:41:44 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-1800/sampler.bin\n",
      "08/20/2024 13:41:44 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-1800/scaler.pt\n",
      "08/20/2024 13:41:44 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-1800/random_states_0.pkl\n",
      "08/20/2024 13:41:44 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-1800\n",
      "08/20/2024 13:49:03 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-2000\n",
      "Model weights saved in models/sd3_exp4/checkpoint-2000/pytorch_lora_weights.safetensors\n",
      "08/20/2024 13:49:04 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-2000/optimizer.bin\n",
      "08/20/2024 13:49:04 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-2000/scheduler.bin\n",
      "08/20/2024 13:49:04 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-2000/sampler.bin\n",
      "08/20/2024 13:49:04 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-2000/scaler.pt\n",
      "08/20/2024 13:49:04 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-2000/random_states_0.pkl\n",
      "08/20/2024 13:49:04 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-2000\n",
      "08/20/2024 13:56:22 - INFO - __main__ - 10 checkpoints already exist, removing 1 checkpoints\n",
      "08/20/2024 13:56:22 - INFO - __main__ - removing checkpoints: checkpoint-200\n",
      "08/20/2024 13:56:22 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-2200\n",
      "Model weights saved in models/sd3_exp4/checkpoint-2200/pytorch_lora_weights.safetensors\n",
      "08/20/2024 13:56:22 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-2200/optimizer.bin\n",
      "08/20/2024 13:56:22 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-2200/scheduler.bin\n",
      "08/20/2024 13:56:22 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-2200/sampler.bin\n",
      "08/20/2024 13:56:22 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-2200/scaler.pt\n",
      "08/20/2024 13:56:22 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-2200/random_states_0.pkl\n",
      "08/20/2024 13:56:22 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-2200\n",
      "08/20/2024 14:03:43 - INFO - __main__ - 10 checkpoints already exist, removing 1 checkpoints\n",
      "08/20/2024 14:03:43 - INFO - __main__ - removing checkpoints: checkpoint-400\n",
      "08/20/2024 14:03:43 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-2400\n",
      "Model weights saved in models/sd3_exp4/checkpoint-2400/pytorch_lora_weights.safetensors\n",
      "08/20/2024 14:03:43 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-2400/optimizer.bin\n",
      "08/20/2024 14:03:43 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-2400/scheduler.bin\n",
      "08/20/2024 14:03:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-2400/sampler.bin\n",
      "08/20/2024 14:03:43 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-2400/scaler.pt\n",
      "08/20/2024 14:03:43 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-2400/random_states_0.pkl\n",
      "08/20/2024 14:03:43 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-2400\n",
      "08/20/2024 14:11:02 - INFO - __main__ - 10 checkpoints already exist, removing 1 checkpoints\n",
      "08/20/2024 14:11:02 - INFO - __main__ - removing checkpoints: checkpoint-600\n",
      "08/20/2024 14:11:02 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-2600\n",
      "Model weights saved in models/sd3_exp4/checkpoint-2600/pytorch_lora_weights.safetensors\n",
      "08/20/2024 14:11:03 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-2600/optimizer.bin\n",
      "08/20/2024 14:11:03 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-2600/scheduler.bin\n",
      "08/20/2024 14:11:03 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-2600/sampler.bin\n",
      "08/20/2024 14:11:03 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-2600/scaler.pt\n",
      "08/20/2024 14:11:03 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-2600/random_states_0.pkl\n",
      "08/20/2024 14:11:03 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-2600\n",
      "08/20/2024 14:18:23 - INFO - __main__ - 10 checkpoints already exist, removing 1 checkpoints\n",
      "08/20/2024 14:18:23 - INFO - __main__ - removing checkpoints: checkpoint-800\n",
      "08/20/2024 14:18:23 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-2800\n",
      "Model weights saved in models/sd3_exp4/checkpoint-2800/pytorch_lora_weights.safetensors\n",
      "08/20/2024 14:18:23 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-2800/optimizer.bin\n",
      "08/20/2024 14:18:23 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-2800/scheduler.bin\n",
      "08/20/2024 14:18:23 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-2800/sampler.bin\n",
      "08/20/2024 14:18:23 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-2800/scaler.pt\n",
      "08/20/2024 14:18:23 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-2800/random_states_0.pkl\n",
      "08/20/2024 14:18:23 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-2800\n",
      "08/20/2024 14:25:45 - INFO - __main__ - 10 checkpoints already exist, removing 1 checkpoints\n",
      "08/20/2024 14:25:45 - INFO - __main__ - removing checkpoints: checkpoint-1000\n",
      "08/20/2024 14:25:45 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-3000\n",
      "Model weights saved in models/sd3_exp4/checkpoint-3000/pytorch_lora_weights.safetensors\n",
      "08/20/2024 14:25:45 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-3000/optimizer.bin\n",
      "08/20/2024 14:25:45 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-3000/scheduler.bin\n",
      "08/20/2024 14:25:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-3000/sampler.bin\n",
      "08/20/2024 14:25:45 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-3000/scaler.pt\n",
      "08/20/2024 14:25:45 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-3000/random_states_0.pkl\n",
      "08/20/2024 14:25:45 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-3000\n",
      "08/20/2024 14:33:05 - INFO - __main__ - 10 checkpoints already exist, removing 1 checkpoints\n",
      "08/20/2024 14:33:05 - INFO - __main__ - removing checkpoints: checkpoint-1200\n",
      "08/20/2024 14:33:05 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-3200\n",
      "Model weights saved in models/sd3_exp4/checkpoint-3200/pytorch_lora_weights.safetensors\n",
      "08/20/2024 14:33:05 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-3200/optimizer.bin\n",
      "08/20/2024 14:33:05 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-3200/scheduler.bin\n",
      "08/20/2024 14:33:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-3200/sampler.bin\n",
      "08/20/2024 14:33:05 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-3200/scaler.pt\n",
      "08/20/2024 14:33:05 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-3200/random_states_0.pkl\n",
      "08/20/2024 14:33:05 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-3200\n",
      "08/20/2024 14:40:26 - INFO - __main__ - 10 checkpoints already exist, removing 1 checkpoints\n",
      "08/20/2024 14:40:26 - INFO - __main__ - removing checkpoints: checkpoint-1400\n",
      "08/20/2024 14:40:26 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-3400\n",
      "Model weights saved in models/sd3_exp4/checkpoint-3400/pytorch_lora_weights.safetensors\n",
      "08/20/2024 14:40:26 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-3400/optimizer.bin\n",
      "08/20/2024 14:40:26 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-3400/scheduler.bin\n",
      "08/20/2024 14:40:26 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-3400/sampler.bin\n",
      "08/20/2024 14:40:26 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-3400/scaler.pt\n",
      "08/20/2024 14:40:26 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-3400/random_states_0.pkl\n",
      "08/20/2024 14:40:26 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-3400\n",
      "08/20/2024 14:47:47 - INFO - __main__ - 10 checkpoints already exist, removing 1 checkpoints\n",
      "08/20/2024 14:47:47 - INFO - __main__ - removing checkpoints: checkpoint-1600\n",
      "08/20/2024 14:47:47 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-3600\n",
      "Model weights saved in models/sd3_exp4/checkpoint-3600/pytorch_lora_weights.safetensors\n",
      "08/20/2024 14:47:47 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-3600/optimizer.bin\n",
      "08/20/2024 14:47:47 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-3600/scheduler.bin\n",
      "08/20/2024 14:47:47 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-3600/sampler.bin\n",
      "08/20/2024 14:47:47 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-3600/scaler.pt\n",
      "08/20/2024 14:47:47 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-3600/random_states_0.pkl\n",
      "08/20/2024 14:47:47 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-3600\n",
      "08/20/2024 14:55:11 - INFO - __main__ - 10 checkpoints already exist, removing 1 checkpoints\n",
      "08/20/2024 14:55:11 - INFO - __main__ - removing checkpoints: checkpoint-1800\n",
      "08/20/2024 14:55:11 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-3800\n",
      "Model weights saved in models/sd3_exp4/checkpoint-3800/pytorch_lora_weights.safetensors\n",
      "08/20/2024 14:55:11 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-3800/optimizer.bin\n",
      "08/20/2024 14:55:11 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-3800/scheduler.bin\n",
      "08/20/2024 14:55:11 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-3800/sampler.bin\n",
      "08/20/2024 14:55:11 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-3800/scaler.pt\n",
      "08/20/2024 14:55:11 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-3800/random_states_0.pkl\n",
      "08/20/2024 14:55:11 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-3800\n",
      "08/20/2024 15:02:32 - INFO - __main__ - 10 checkpoints already exist, removing 1 checkpoints\n",
      "08/20/2024 15:02:32 - INFO - __main__ - removing checkpoints: checkpoint-2000\n",
      "08/20/2024 15:02:32 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-4000\n",
      "Model weights saved in models/sd3_exp4/checkpoint-4000/pytorch_lora_weights.safetensors\n",
      "08/20/2024 15:02:32 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-4000/optimizer.bin\n",
      "08/20/2024 15:02:32 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-4000/scheduler.bin\n",
      "08/20/2024 15:02:32 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-4000/sampler.bin\n",
      "08/20/2024 15:02:32 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-4000/scaler.pt\n",
      "08/20/2024 15:02:32 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-4000/random_states_0.pkl\n",
      "08/20/2024 15:02:32 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-4000\n",
      "08/20/2024 15:09:52 - INFO - __main__ - 10 checkpoints already exist, removing 1 checkpoints\n",
      "08/20/2024 15:09:52 - INFO - __main__ - removing checkpoints: checkpoint-2200\n",
      "08/20/2024 15:09:52 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-4200\n",
      "Model weights saved in models/sd3_exp4/checkpoint-4200/pytorch_lora_weights.safetensors\n",
      "08/20/2024 15:09:52 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-4200/optimizer.bin\n",
      "08/20/2024 15:09:52 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-4200/scheduler.bin\n",
      "08/20/2024 15:09:52 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-4200/sampler.bin\n",
      "08/20/2024 15:09:52 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-4200/scaler.pt\n",
      "08/20/2024 15:09:52 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-4200/random_states_0.pkl\n",
      "08/20/2024 15:09:52 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-4200\n",
      "08/20/2024 15:17:16 - INFO - __main__ - 10 checkpoints already exist, removing 1 checkpoints\n",
      "08/20/2024 15:17:16 - INFO - __main__ - removing checkpoints: checkpoint-2400\n",
      "08/20/2024 15:17:16 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-4400\n",
      "Model weights saved in models/sd3_exp4/checkpoint-4400/pytorch_lora_weights.safetensors\n",
      "08/20/2024 15:17:16 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-4400/optimizer.bin\n",
      "08/20/2024 15:17:16 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-4400/scheduler.bin\n",
      "08/20/2024 15:17:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-4400/sampler.bin\n",
      "08/20/2024 15:17:16 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-4400/scaler.pt\n",
      "08/20/2024 15:17:16 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-4400/random_states_0.pkl\n",
      "08/20/2024 15:17:16 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-4400\n",
      "08/20/2024 15:24:37 - INFO - __main__ - 10 checkpoints already exist, removing 1 checkpoints\n",
      "08/20/2024 15:24:37 - INFO - __main__ - removing checkpoints: checkpoint-2600\n",
      "08/20/2024 15:24:37 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-4600\n",
      "Model weights saved in models/sd3_exp4/checkpoint-4600/pytorch_lora_weights.safetensors\n",
      "08/20/2024 15:24:37 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-4600/optimizer.bin\n",
      "08/20/2024 15:24:37 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-4600/scheduler.bin\n",
      "08/20/2024 15:24:37 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-4600/sampler.bin\n",
      "08/20/2024 15:24:37 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-4600/scaler.pt\n",
      "08/20/2024 15:24:37 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-4600/random_states_0.pkl\n",
      "08/20/2024 15:24:37 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-4600\n",
      "08/20/2024 15:31:59 - INFO - __main__ - 10 checkpoints already exist, removing 1 checkpoints\n",
      "08/20/2024 15:31:59 - INFO - __main__ - removing checkpoints: checkpoint-2800\n",
      "08/20/2024 15:31:59 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-4800\n",
      "Model weights saved in models/sd3_exp4/checkpoint-4800/pytorch_lora_weights.safetensors\n",
      "08/20/2024 15:31:59 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-4800/optimizer.bin\n",
      "08/20/2024 15:31:59 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-4800/scheduler.bin\n",
      "08/20/2024 15:31:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-4800/sampler.bin\n",
      "08/20/2024 15:31:59 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-4800/scaler.pt\n",
      "08/20/2024 15:31:59 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-4800/random_states_0.pkl\n",
      "08/20/2024 15:31:59 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-4800\n",
      "08/20/2024 15:39:18 - INFO - __main__ - 10 checkpoints already exist, removing 1 checkpoints\n",
      "08/20/2024 15:39:18 - INFO - __main__ - removing checkpoints: checkpoint-3000\n",
      "08/20/2024 15:39:18 - INFO - accelerate.accelerator - Saving current state to models/sd3_exp4/checkpoint-5000\n",
      "Model weights saved in models/sd3_exp4/checkpoint-5000/pytorch_lora_weights.safetensors\n",
      "08/20/2024 15:39:19 - INFO - accelerate.checkpointing - Optimizer state saved in models/sd3_exp4/checkpoint-5000/optimizer.bin\n",
      "08/20/2024 15:39:19 - INFO - accelerate.checkpointing - Scheduler state saved in models/sd3_exp4/checkpoint-5000/scheduler.bin\n",
      "08/20/2024 15:39:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/sd3_exp4/checkpoint-5000/sampler.bin\n",
      "08/20/2024 15:39:19 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/sd3_exp4/checkpoint-5000/scaler.pt\n",
      "08/20/2024 15:39:19 - INFO - accelerate.checkpointing - Random states saved in models/sd3_exp4/checkpoint-5000/random_states_0.pkl\n",
      "08/20/2024 15:39:19 - INFO - __main__ - Saved state to models/sd3_exp4/checkpoint-5000\n",
      "Model weights saved in models/sd3_exp4/pytorch_lora_weights.safetensors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e25bca17c84a3db2e4edecc66a52b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded transformer as SD3Transformer2DModel from `transformer` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded text_encoder as CLIPTextModelWithProjection from `text_encoder` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "{'max_shift', 'base_image_seq_len', 'max_image_seq_len', 'base_shift', 'use_dynamic_shifting'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "{'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Loaded tokenizer_3 as T5TokenizerFast from `tokenizer_3` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe8f2b0073e4b9e85e14b3d541de802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded text_encoder_3 as T5EncoderModel from `text_encoder_3` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\n",
    "logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "global_step = 0\n",
    "first_epoch = 0\n",
    "initial_global_step = 0\n",
    "\n",
    "progress_bar = tqdm(\n",
    "    range(0, args.max_train_steps),\n",
    "    initial=initial_global_step,\n",
    "    desc=\"Steps\",\n",
    "    # Only show the progress bar once on each machine.\n",
    "    disable=not accelerator.is_local_main_process,\n",
    ")\n",
    "\n",
    "for epoch in range(first_epoch, args.num_train_epochs):\n",
    "    transformer.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        models_to_accumulate = [transformer]\n",
    "        with accelerator.accumulate(models_to_accumulate):\n",
    "            pixel_values = batch[\"pixel_values\"].to(dtype=vae.dtype)\n",
    "            prompts = batch[\"prompts\"]\n",
    "            prompt_embeds, pooled_prompt_embeds = batch[\"embeddings\"], batch[\"pooled_embeddings\"]\n",
    "                        \n",
    "            # Convert images to latent space\n",
    "            model_input = vae.encode(pixel_values).latent_dist.sample()\n",
    "            model_input = (model_input - vae.config.shift_factor) * vae.config.scaling_factor\n",
    "            model_input = model_input.to(dtype=weight_dtype)\n",
    "\n",
    "            # Sample noise that we'll add to the latents\n",
    "            noise = torch.randn_like(model_input)\n",
    "            bsz = model_input.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            # for weighting schemes where we sample timesteps non-uniformly\n",
    "            u = compute_density_for_timestep_sampling(\n",
    "                weighting_scheme=args.weighting_scheme,\n",
    "                batch_size=bsz,\n",
    "                logit_mean=args.logit_mean,\n",
    "                logit_std=args.logit_std,\n",
    "                mode_scale=args.mode_scale,\n",
    "            )\n",
    "            indices = (u * noise_scheduler_copy.config.num_train_timesteps).long()\n",
    "            timesteps = noise_scheduler_copy.timesteps[indices].to(device=model_input.device)\n",
    "\n",
    "            # Add noise according to flow matching.\n",
    "            # zt = (1 - texp) * x + texp * z1\n",
    "            sigmas = get_sigmas(timesteps, n_dim=model_input.ndim, dtype=model_input.dtype)\n",
    "            noisy_model_input = (1.0 - sigmas) * model_input + sigmas * noise\n",
    "\n",
    "            # Predict the noise residual\n",
    "            model_pred = transformer(\n",
    "                hidden_states=noisy_model_input,\n",
    "                timestep=timesteps,\n",
    "                encoder_hidden_states=prompt_embeds,\n",
    "                pooled_projections=pooled_prompt_embeds,\n",
    "                return_dict=False,\n",
    "            )[0]\n",
    "\n",
    "            # Follow: Section 5 of https://arxiv.org/abs/2206.00364.\n",
    "            # Preconditioning of the model outputs.\n",
    "            if args.precondition_outputs:\n",
    "                model_pred = model_pred * (-sigmas) + noisy_model_input\n",
    "\n",
    "            # these weighting schemes use a uniform timestep sampling\n",
    "            # and instead post-weight the loss\n",
    "            weighting = compute_loss_weighting_for_sd3(weighting_scheme=args.weighting_scheme, sigmas=sigmas)\n",
    "\n",
    "            # flow matching loss\n",
    "            if args.precondition_outputs:\n",
    "                target = model_input\n",
    "            else:\n",
    "                target = noise - model_input\n",
    "\n",
    "            # Compute regular loss.\n",
    "            loss = torch.mean(\n",
    "                (weighting.float() * (model_pred.float() - target.float()) ** 2).reshape(target.shape[0], -1),\n",
    "                1,\n",
    "            )\n",
    "            loss = loss.mean()\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                params_to_clip = transformer_lora_parameters\n",
    "                accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "\n",
    "            if accelerator.is_main_process:\n",
    "                if global_step % args.checkpointing_steps == 0:\n",
    "                    # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\n",
    "                    if args.checkpoints_total_limit is not None:\n",
    "                        checkpoints = os.listdir(args.output_dir)\n",
    "                        checkpoints = [d for d in checkpoints if d.startswith(\"checkpoint\")]\n",
    "                        checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "\n",
    "                        # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit - 1` checkpoints\n",
    "                        if len(checkpoints) >= args.checkpoints_total_limit:\n",
    "                            num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1\n",
    "                            removing_checkpoints = checkpoints[0:num_to_remove]\n",
    "\n",
    "                            logger.info(\n",
    "                                f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\"\n",
    "                            )\n",
    "                            logger.info(f\"removing checkpoints: {', '.join(removing_checkpoints)}\")\n",
    "\n",
    "                            for removing_checkpoint in removing_checkpoints:\n",
    "                                removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)\n",
    "                                shutil.rmtree(removing_checkpoint)\n",
    "\n",
    "                    save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
    "                    accelerator.save_state(save_path)\n",
    "                    logger.info(f\"Saved state to {save_path}\")\n",
    "\n",
    "        logs = {\"train_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        accelerator.log(logs, step=global_step)\n",
    "\n",
    "        if global_step >= args.max_train_steps:\n",
    "            break\n",
    "\n",
    "# Save the lora layers\n",
    "accelerator.wait_for_everyone()\n",
    "if accelerator.is_main_process:\n",
    "    transformer = unwrap_model(transformer)\n",
    "    transformer = transformer.to(torch.float32)\n",
    "    transformer_lora_layers = get_peft_model_state_dict(transformer)\n",
    "\n",
    "    text_encoder_lora_layers = None\n",
    "    text_encoder_2_lora_layers = None\n",
    "\n",
    "    StableDiffusion3Pipeline.save_lora_weights(\n",
    "        save_directory=args.output_dir,\n",
    "        transformer_lora_layers=transformer_lora_layers,\n",
    "        text_encoder_lora_layers=text_encoder_lora_layers,\n",
    "        text_encoder_2_lora_layers=text_encoder_2_lora_layers,\n",
    "    )\n",
    "\n",
    "    # Final inference\n",
    "    # Load previous pipeline\n",
    "    pipeline = StableDiffusion3Pipeline.from_pretrained(\n",
    "        args.pretrained_model_name_or_path,\n",
    "        revision=args.revision,\n",
    "        variant=args.variant,\n",
    "        torch_dtype=weight_dtype,\n",
    "    )\n",
    "    # load attention processors\n",
    "    pipeline.load_lora_weights(args.output_dir)\n",
    "\n",
    "accelerator.end_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dif_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
